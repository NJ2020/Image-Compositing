{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7IlTi39Oj9u8"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf; tf.set_random_seed(42);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "naV0oKWgkGvD"
   },
   "outputs": [],
   "source": [
    "def conv_2d(x, kernel_size = 4, stride = 1, out_channels = 64, is_conv = True, is_norm = True, normalization = 'instance', \n",
    "            is_act = True, activation = 'lrelu', leak_param = 0.2, padding = 'VALID'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x:             Input Tensor\n",
    "    kernel_size:   Integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window\n",
    "    stride:        Integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width\n",
    "    out_channels:  Integer, the dimensionality of the output space\n",
    "    is_conv:       Boolean, whether to perform convolution or not\n",
    "    is_norm:       Boolean, whether to normalize the data or not\n",
    "    normalization: Type of normalization, supports batch and instance normalization\n",
    "    is_act:        Boolean, whether to apply non-linear activation functions or not\n",
    "    activation:    Type of activation function, supports Leaky_ReLU, ReLU and Sigmoid and Tanh\n",
    "    leak_param:    Integer, Leakiness to use in case of Leaky ReLU\n",
    "    padding:       Zero padding around the image, supports VALID and SAME\n",
    "    \n",
    "    Returns:\n",
    "    x:             Output Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "#   Apply Non-linearities\n",
    "    if is_act == True:\n",
    "        if activation == 'lrelu': x = tf.nn.leaky_relu(x, leak_param)\n",
    "        elif activation == 'relu': x = tf.nn.relu(x);\n",
    "        elif activation == 'sigmoid': x = tf.nn.sigmoid(x)\n",
    "        elif activation == 'tanh': x = tf.nn.tanh(x)\n",
    "        else: print('Check you activation function again in Conv block!!')\n",
    "    \n",
    "#   Apply Convolution after doing mirror padding keeping the output size same as that of an input\n",
    "    if is_conv == True:\n",
    "        if(padding == 'VALID'):     \n",
    "            x = tf.pad(x, [[0, 0], [(kernel_size-1)//2, (kernel_size-1)//2], [(kernel_size-1)//2, (kernel_size-1)//2], [0, 0]],\n",
    "                       mode = 'REFLECT')         \n",
    "        x = tf.layers.conv2d(x, filters = out_channels, kernel_size = kernel_size, strides = stride, padding = padding, \n",
    "                             use_bias = False, kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "        \n",
    "#   Apply Normalization\n",
    "    if is_norm == True:\n",
    "        if(normalization == 'instance'): x = tf.contrib.layers.instance_norm(x, epsilon = 1e-6)\n",
    "        elif(normalization == 'batch'): x = tf.contrib.layers.batch_norm(x, epsilon = 1e-5, training = train_mode, \n",
    "                                                                         momentum = 0.9)\n",
    "        else: print('Check your normalization function again !!')\n",
    "    \n",
    "    return x\n",
    "\n",
    "  \n",
    "def conv_2d_transpose(x, kernel_size, stride, out_channels, is_deconv = True, is_act = True, activation = 'relu', \n",
    "                      leak_param = 0.2, is_norm = True, normalization = 'instance', is_dropout = False, dropout = 0.5):\n",
    "  \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    x:             Input Tensor\n",
    "    kernel_size:   Integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window\n",
    "    stride:        Integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width\n",
    "    out_channels:  Integer, the dimensionality of the output space\n",
    "    is_deconv:       Boolean, whether to perform convolution or not\n",
    "    is_norm:       Boolean, whether to normalize the data or not\n",
    "    normalization: Type of normalization, supports batch and instance normalization\n",
    "    is_act:        Boolean, whether to apply non-linear activation functions or not\n",
    "    activation:    Type of activation function, supports Leaky_ReLU, ReLU and Sigmoid and Tanh\n",
    "    leak_param:    Integer, Leakiness to use in case of Leaky ReLU\n",
    "    is_dropout:    Boolean, whether to apply add dropout layer or not\n",
    "    dropout:       A scalar Tensor with the same type as x. The probability that each element is kept.\n",
    "    \n",
    "    Returns:\n",
    "    x:             Output Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "#   Apply Non-linearities\n",
    "    if is_act == True:\n",
    "        if activation == 'lrelu': x = tf.nn.leaky_relu(x, leak_param)\n",
    "        elif activation == 'relu': x = tf.nn.relu(x)\n",
    "        elif activation == 'sigmoid': x = tf.nn.sigmoid(x)\n",
    "        elif activation == 'tanh': x = tf.nn.tanh(x)\n",
    "        else: print('Check you activation function again in Conv block!!')\n",
    "\n",
    "#   Apply Deconvolution\n",
    "    if is_deconv == True:\n",
    "        x = tf.layers.conv2d_transpose(x, filters = out_channels, kernel_size = kernel_size, strides = stride, \n",
    "            padding = 'SAME', use_bias = False, kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d())\n",
    "\n",
    "#   Apply Normalization\n",
    "    if is_norm == True:\n",
    "        if(normalization == 'instance'): x = tf.contrib.layers.instance_norm(x, epsilon = 1e-6)\n",
    "        elif(normalization == 'batch'): x = tf.contrib.layers.batch_norm(x, epsilon = 1e-5, training = train_mode,\n",
    "                                                                        momentum = 0.9)\n",
    "        else: print('Check your normalization function again !!')\n",
    "    \n",
    "#   Add Dropout layer\n",
    "    if is_dropout == True: x = tf.nn.dropout(x, keep_prob = 1-dropout)\n",
    "    \n",
    "#   NOTE: We don't want to turn off the dropout at the validation/test time in order to bring out the diversity in the images\n",
    "#         generated by Generator. Moreover, instance normalization works best in case of GANs. Due to all these reasons, \n",
    "#         I didn't use placeholders for the boolean (dropout mode)!!\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JdYH9FQfpXy4"
   },
   "outputs": [],
   "source": [
    "def discriminator_patch_gan(input_ten, target_ten, out_channels = 64, use_sigmoid = False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_ten:    Input Tensor\n",
    "    target_ten:   Target Tensor\n",
    "    out_channels: Integer, the dimensionality of the output space\n",
    "    use_sigmoid:  Boolean, whether to use sigmoid at the last layer or not\n",
    "    \n",
    "    Returns:\n",
    "    x:            Output Tensor\n",
    "    \"\"\"\n",
    "    \n",
    "#   Patch_gan discriminator is Conditional GAN, so training it requires conditionality on the input_tensor \n",
    "#   [image from the first domain which should get converted to second domain]\n",
    "    x = tf.concat([input_ten, target_ten], axis = 3)\n",
    "\n",
    "#   Number of convolutional layers used should be such that, the receptive field is 70*70\n",
    "#   Authors tried out many different possible variants, but this config worked best for them!!\n",
    "#   No normalization in the first layer\n",
    "\n",
    "    x = conv_2d(x, 4, 2, out_channels, is_norm = False, is_act = False)\n",
    "    x = conv_2d(x, 4, 2, out_channels*2); x = conv_2d(x, 4, 2, out_channels*4)\n",
    "    x = conv_2d(x, 4, 2, out_channels*8); x = conv_2d(x, 4, 1, out_channels*8)\n",
    "    \n",
    "#   No normalization in the last layer\n",
    "    x = conv_2d(x, 4, 1, 1, is_norm = False)\n",
    "    \n",
    "#   Only use sigmoid when probabilities are needed [generally we need un-normalized logits for the loss function]\n",
    "    if use_sigmoid == True:\n",
    "        x = tf.nn.sigmoid(x); print('Sigmoid activation in the discriminator')\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EZu0UjwdqRpr"
   },
   "outputs": [],
   "source": [
    "def generator_unet_128(input_ten, out_channels = 64):\n",
    "  \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_ten:    Input Tensor\n",
    "    out_channels: Integer, the dimensionality of the output space\n",
    "    \n",
    "    Returns:\n",
    "    o_2:          Output Tensor with tanh activation function applied\n",
    "    \"\"\"\n",
    "    \n",
    "#   Two layers of vanilla convolution in the initial part of the architecture just to increase the receptive field\n",
    "#   No batch norm layer just after the input layer!\n",
    "    i_1 = conv_2d(input_ten, 3, 1, out_channels*(1), is_norm = False, is_act = False)\n",
    "    i_2 = conv_2d(i_1, 3, 1, out_channels*(1));\n",
    "    \n",
    "#   Architecture of the encoder [All are stride 2 convolutions]\n",
    "    e_1 = conv_2d(i_2, 4, 2, out_channels*(1)); e_2 = conv_2d(e_1, 4, 2, out_channels*(2))\n",
    "    e_3 = conv_2d(e_2, 4, 2, out_channels*(4)); e_4 = conv_2d(e_3, 4, 2, out_channels*(8))\n",
    "    e_5 = conv_2d(e_4, 4, 2, out_channels*(8)); e_6 = conv_2d(e_5, 4, 2, out_channels*(8))\n",
    "    e_7 = conv_2d(e_6, 4, 2, out_channels*(8), is_norm = False)\n",
    "\n",
    "#   Architecture of the decoder [Adding Skip connections between the layers of encoder and decoder of same dimensionality]\n",
    "#   NOTE: U-Net preserves the fine granularity in the output that will otherwise be lost if using stride 2 conv layers!! \n",
    "    d_1 = conv_2d_transpose(e_7, 4, 2, out_channels*(8), is_dropout = True); d_1 = tf.concat([d_1, e_6], axis = 3)\n",
    "    d_2 = conv_2d_transpose(d_1, 4, 2, out_channels*(8), is_dropout = True); d_2 = tf.concat([d_2, e_5], axis = 3)\n",
    "    d_3 = conv_2d_transpose(d_2, 4, 2, out_channels*(8), is_dropout = True); d_3 = tf.concat([d_3, e_4], axis = 3)\n",
    "    d_4 = conv_2d_transpose(d_3, 4, 2, out_channels*(4)); d_4 = tf.concat([d_4, e_3], axis = 3)\n",
    "    d_5 = conv_2d_transpose(d_4, 4, 2, out_channels*(2)); d_5 = tf.concat([d_5, e_2], axis = 3)\n",
    "    d_6 = conv_2d_transpose(d_5, 4, 2, out_channels*(1)); d_6 = tf.concat([d_6, e_1], axis = 3)\n",
    "    d_7 = conv_2d_transpose(d_6, 4, 2, out_channels*(1))\n",
    "    \n",
    "#   Last few vanilla convolutional layers without any stridding\n",
    "    o_1 = conv_2d_transpose(d_7, 4, 1, out_channels*(1)); o_2 = conv_2d_transpose(o_1, 4, 1, 3, is_norm = False)\n",
    "#   Tanh activation function to ensure the range of output to be inbetween -1 and 1!\n",
    "    o_2 = tf.nn.tanh(o_2)\n",
    "\n",
    "    return o_2\n",
    "\n",
    "  \n",
    "def generator_unet_256(input_ten, out_channels = 64):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_ten:    Input Tensor\n",
    "    out_channels: Integer, the dimensionality of the output space\n",
    "    \n",
    "    Returns:\n",
    "    o_3:          Output Tensor with tanh activation function applied\n",
    "    \"\"\"\n",
    "    \n",
    "#   Few layers of vanilla convolution in the initial part of the architecture just to increase the receptive field\n",
    "#   No batch norm layer just after the input layer!\n",
    "    i_1 = conv_2d(input_ten, 3, 1, out_channels*(1), is_norm = False, is_act = False)\n",
    "    i_2 = conv_2d(i_1, 3, 1, out_channels*(1)); i_3 = conv_2d(i_2, 3, 1, out_channels*(2))\n",
    "\n",
    "#   Architecture of the encoder [All are stride 2 convolutions]\n",
    "    e_1 = conv_2d(i_3, 4, 2, out_channels*(2)); e_2 = conv_2d(e_1, 4, 2, out_channels*(4))\n",
    "    e_3 = conv_2d(e_2, 4, 2, out_channels*(4)); e_4 = conv_2d(e_3, 4, 2, out_channels*(8))\n",
    "    e_5 = conv_2d(e_4, 4, 2, out_channels*(8)); e_6 = conv_2d(e_5, 4, 2, out_channels*(8))\n",
    "    e_7 = conv_2d(e_6, 4, 2, out_channels*(8))\n",
    "    e_8 = conv_2d(e_7, 4, 2, out_channels*(8), is_norm = False)\n",
    "\n",
    "#   Architecture of the decoder [Adding Skip coonections between the layers of encoder and decoder of same dimensionality]\n",
    "#   NOTE: U-Net preserves the fine granularity in the output that will otherwise be lost if using stride 2 conv layers!!\n",
    "    d_1 = conv_2d_transpose(e_8, 4, 2, out_channels*(8), is_dropout = True); d_1 = tf.concat([d_1, e_7], axis = 3)\n",
    "    d_2 = conv_2d_transpose(d_1, 4, 2, out_channels*(8), is_dropout = True); d_2 = tf.concat([d_2, e_6], axis = 3)\n",
    "    d_3 = conv_2d_transpose(d_2, 4, 2, out_channels*(8), is_dropout = True); d_3 = tf.concat([d_3, e_5], axis = 3)\n",
    "    d_4 = conv_2d_transpose(d_3, 4, 2, out_channels*(8)); d_4 = tf.concat([d_4, e_4], axis = 3)\n",
    "    d_5 = conv_2d_transpose(d_4, 4, 2, out_channels*(4)); d_5 = tf.concat([d_5, e_3], axis = 3)\n",
    "    d_6 = conv_2d_transpose(d_5, 4, 2, out_channels*(4)); d_6 = tf.concat([d_6, e_2], axis = 3)\n",
    "    d_7 = conv_2d_transpose(d_6, 4, 2, out_channels*(2)); d_7 = tf.concat([d_7, e_1], axis = 3)\n",
    "    d_8 = conv_2d_transpose(d_7, 4, 2, out_channels*(2))\n",
    "    \n",
    "#   Last few vanilla convolutional layers without any stridding\n",
    "    o_1 = conv_2d_transpose(d_8, 4, 1, out_channels*(1)); o_2 = conv_2d_transpose(o_1, 4, 1, out_channels*(1)) \n",
    "    o_3 = conv_2d_transpose(o_2, 4, 1, 3, is_norm = False); \n",
    "    \n",
    "#   Tanh activation function to ensure the range of output to be inbetween -1 and 1!\n",
    "    o_3 = tf.nn.tanh(o_3)\n",
    "\n",
    "    return o_3\n",
    "\n",
    "\n",
    "def generator_unet_512(input_ten, out_channels = 64):\n",
    "    \n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    input_ten:    Input Tensor\n",
    "    out_channels: Integer, the dimensionality of the output space\n",
    "    \n",
    "    Returns:\n",
    "    o_4:          Output Tensor with tanh activation function applied\n",
    "    \"\"\"\n",
    "    \n",
    "#   Few layers of vanilla convolution in the initial part of the architecture just to increase the receptive field\n",
    "#   No batch norm layer just after the input layer!\n",
    "    i_1 = conv_2d(input_ten, 3, 1, out_channels*(1), is_norm = False, is_act = False)\n",
    "    i_2 = conv_2d(i_1, 3, 1, out_channels*(1)); i_3 = conv_2d(i_2, 3, 1, out_channels*(2))\n",
    "    i_4 = conv_2d(i_3, 3, 1, out_channels*(2))\n",
    "    \n",
    "#   Architecture of the encoder [All are stride 2 convolutions]\n",
    "    e_1 = conv_2d(i_4, 4, 2, out_channels*(4)); e_2 = conv_2d(e_1, 4, 2, out_channels*(4))\n",
    "    e_3 = conv_2d(e_2, 4, 2, out_channels*(4)); e_4 = conv_2d(e_3, 4, 2, out_channels*(8))\n",
    "    e_5 = conv_2d(e_4, 4, 2, out_channels*(8)); e_6 = conv_2d(e_5, 4, 2, out_channels*(8))\n",
    "    e_7 = conv_2d(e_6, 4, 2, out_channels*(8)); e_8 = conv_2d(e_7, 4, 2, out_channels*(16))\n",
    "    e_9 = conv_2d(e_8, 4, 2, out_channels*(16), is_norm = False)\n",
    "\n",
    "#   Architecture of the decoder [Adding Skip coonections between the layers of encoder and decoder of same dimensionality]\n",
    "#   NOTE: U-Net preserves the fine granularity in the output that will otherwise be lost if using stride 2 conv layers!! \n",
    "    d_1 = conv_2d_transpose(e_9, 4, 2, out_channels*(16), is_dropout = True); d_1 = tf.concat([d_1, e_8], axis = 3)\n",
    "    d_2 = conv_2d_transpose(d_1, 4, 2, out_channels*(8), is_dropout = True); d_2 = tf.concat([d_2, e_7], axis = 3)\n",
    "    d_3 = conv_2d_transpose(d_2, 4, 2, out_channels*(8), is_dropout = True); d_3 = tf.concat([d_3, e_6], axis = 3)\n",
    "    d_4 = conv_2d_transpose(d_3, 4, 2, out_channels*(8), is_dropout = True); d_4 = tf.concat([d_4, e_5], axis = 3)\n",
    "    d_5 = conv_2d_transpose(d_4, 4, 2, out_channels*(8)); d_5 = tf.concat([d_5, e_4], axis = 3)\n",
    "    d_6 = conv_2d_transpose(d_5, 4, 2, out_channels*(4)); d_6 = tf.concat([d_6, e_3], axis = 3)\n",
    "    d_7 = conv_2d_transpose(d_6, 4, 2, out_channels*(4)); d_7 = tf.concat([d_7, e_2], axis = 3)\n",
    "    d_8 = conv_2d_transpose(d_7, 4, 2, out_channels*(4)); d_8 = tf.concat([d_8, e_1], axis = 3)\n",
    "    d_9 = conv_2d_transpose(d_8, 4, 2, out_channels*(2))\n",
    "    \n",
    "#   Last few vanilla convolutional layers without any stridding\n",
    "    o_1 = conv_2d_transpose(d_9, 4, 1, out_channels*(2)); o_2 = conv_2d_transpose(o_1, 4, 1, out_channels*(1))\n",
    "    o_3 = conv_2d_transpose(o_2, 4, 1, out_channels*(1)); o_4 = conv_2d_transpose(o_3, 4, 1, 3, is_norm = False)\n",
    "    \n",
    "#   Tanh activation function to ensure the range of output to be inbetween -1 and 1!\n",
    "    o_4 = tf.nn.tanh(o_4)\n",
    "\n",
    "    return o_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4BzKzWXIr-Qs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Architecture",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
